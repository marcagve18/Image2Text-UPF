# Image2Text-UPF

The goal of this project is to understand how image captioning models work, implement some architectures to perform image captioning and experiment with some well known frameworks to test the performance of the resulting models.

## 1. Introduction to image captioning

Image captining consists on generating a textual description of an input image. Typical deep learing (DL) architectures that aim to solve this task are divided into two main components:
- An **encoder**, which is in charge of extracting the relevant features of the input image.
- A **decoder**, which uses the extracted features to build the final sentence describing the original image.

The most basic architectures use a CNN-RNN approach. However, more recent and complex architectures have been proven to have impressive results. This is the case of attention-based methods

...

## 2. Dataset

### 2.1 Transformations we have made prior to training the model

## 3. VGG & GPT2

## 4. CNN & RNN

## 5. Nocaps

### 5.1. Preprocessing

### 5.2. Metrics

### 5.3. Results

## 6. Conclusions

